{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platform\n",
    "## Assignment 2: MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By:**  \n",
    "\n",
    "Omri Newman, 806646    \n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this assignment is to:**\n",
    "- Understand and practice the details of MapReduceEngine\n",
    "\n",
    "**Instructions:**\n",
    "- Students will form teams of two people each, and submit a single homework for each team.\n",
    "- The same score for the homework will be given to each member of your team.\n",
    "- Your solution is in the form of a Jupyter notebook file (with extension ipynb).\n",
    "- Images/Graphs/Tables should be submitted inside the notebook.\n",
    "- The notebook should be runnable and properly documented. \n",
    "- Please answer all the questions and include all your code.\n",
    "- You are expected to submit a clear and pythonic code.\n",
    "- You can change functions signatures/definitions.\n",
    "\n",
    "**Submission:**\n",
    "- Submission of the homework will be done via Moodle by uploading a Jupyter notebook.\n",
    "- The homework needs to be entirely in English.\n",
    "- The deadline for submission is on Moodle.\n",
    "- Late submission won't be allowed.\n",
    "  \n",
    "  \n",
    "- In case of identical code submissions - both groups will get a Zero. \n",
    "- Some groups might be selected randomly to present their code.\n",
    "\n",
    "**Requirements:**  \n",
    "- Python 3.6 should be used.  \n",
    "- You should implement the algorithms by yourself using only basic Python libraries (such as numpy,pandas,etc.)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading:**\n",
    "- Q1 - 5 points - Initial Steps\n",
    "- Q2 - 50 points - MapReduceEngine\n",
    "- Q3 - 30 points - Implement the MapReduce Inverted index of the JSON documents\n",
    "- Q4 - 5 points - Testing Your MapReduce\n",
    "- Q5 - 10 points - Final Thoughts \n",
    "\n",
    "`Total: 100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "# !pip install --quiet zipfile36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import threading # you can use easier threading packages\n",
    "import concurrent\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "# ml\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# visual\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# notebook\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hide Warnings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disable Autoscrolling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Random Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 1\n",
    "# Initial Steps\n",
    "\n",
    "Write Python code to create 20 different CSV files in this format:  `myCSV[Number].csv`, where each file contains 10 records. \n",
    "\n",
    "The schema is `(‘firstname’,’secondname’,city’)`  \n",
    "\n",
    "Values should be randomly chosen from the lists: \n",
    "- `firstname` : `[John, Dana, Scott, Marc, Steven, Michael, Albert, Johanna]`  \n",
    "- `city` : `[New York, Haifa, München, London, Palo Alto,  Tel Aviv, Kiel, Hamburg]`  \n",
    "- `secondname`: any value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstname = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "city = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto',  'TelAviv', 'Kiel', 'Hamburg']\n",
    "secondname = ['Taggar', 'Newman', 'Berl', 'Fogel', 'Shapira', 'Levi', 'Cohen'] # please use some version of random\n",
    "\n",
    "records = 10\n",
    "for i in range(1, 21):\n",
    "    pd.DataFrame(data={'firstname': np.random.choice(firstname, records),\n",
    "                 'secondname': np.random.choice(secondname, records),\n",
    "                 'city': np.random.choice(city, records)}).to_csv(f'myCSV{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python to Create `mapreducetemp` and `mapreducefinal` folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('mapreducetemp')\n",
    "os.mkdir('mapreducefinal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Question 2\n",
    "## MapReduceEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Python code to create an SQLite database with the following table\n",
    "\n",
    "`TableName: temp_results`   \n",
    "`schema: (key:TEXT,value:TEXT)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('mydb.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conn.execute('''CREATE TABLE IF NOT EXISTS temp_results(\n",
    "                    key VARCHAR(20),\n",
    "                    value VARCHAR(20)\n",
    "                    )''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [key, value]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql('SELECT * FROM temp_results', conn).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Create a Python class** `MapReduceEngine` with method `def execute(input_data, map_function, reduce_function)`, such that:\n",
    "    - `input_data`: is an array of elements\n",
    "    - `map_function`: is a pointer to the Python function that returns a list where each entry of the form (key,value) \n",
    "    - `reduce_function`: is pointer to the Python function that returns a list where each entry of the form (key,value)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Implement** the following functionality in the `execute(...)` function:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. For each key  from the  input_data, start a new Python thread that executes map_function(key) \n",
    "<br><br>\n",
    "2. Each thread will store results of the map_function into mapreducetemp/part-tmp-X.csv where X is a unique number per each thread.\n",
    "<br><br>\n",
    "3. Keep the list of all threads and check whether they are completed.\n",
    "<br><br>\n",
    "4. Once all threads completed, load content of all CSV files into the temp_results table in SQLite.\n",
    "\n",
    "    Remark: Easiest way to loop over all CSV files and load them into Pandas first, then load into SQLite  \n",
    "    `data = pd.read_csv(path to csv)`  \n",
    "    `data.to_sql(‘temp_results’,sql_conn, if_exists=’append’,index=False)`\n",
    "<br><br>\n",
    "\n",
    "5. **Write SQL statement** that generates a sorted list by key of the form `(key, value)` where value is concatenation of ALL values in the value column that match specific key. For example, if table has records\n",
    "<table>\n",
    "    <tbody>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV1.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">Dana</td>\n",
    "                <td style=\"text-align:center\">myCSV5.csv</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"text-align:center\">John</td>\n",
    "                <td style=\"text-align:center\">myCSV7.csv</td>\n",
    "            </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "    Then SQL statement will return `(‘John’,’myCSV1.csv, myCSV7.csv’)`  \n",
    "    Remark: use GROUP_CONCAT and also GROUP BY ORDER BY\n",
    "<br><br><br>\n",
    "6. **Start a new thread** for each value from the generated list in the previous step, to execute `reduce_function(key,value)` \n",
    "<br>    \n",
    "7. Each thread will store results of reduce_function into `mapreducefinal/part-X-final.csv` file  \n",
    "<br>\n",
    "8. Keep list of all threads and check whether they are completed  \n",
    "<br>\n",
    "9. Once all threads completed, print on the screen `MapReduce Completed` otherwise print `MapReduce Failed` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** - We choose to initiate the threads using `concurrent.features`. The `with` statement ensures all threads will finish before exiting and continuing to the next step. Still, we define the `check_threads` function which will iterate over all threads and check if they successfuly finished their tasks. Each of the task functions (write_tmp, write_final) should return `True`, hence checking if the threads finished successfuly is equivalent to checking if each thread holds the value `True`. We check for the threads after the mapping step and after the reducing step.\n",
    "\n",
    "We have an optional argument (`logs`) which will print a live log of the threads. We included a sample of that in a bonus section in Q4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement all of the class here\n",
    "\n",
    "class MapReduceEngine():\n",
    "    def write_tmp(inputs, thread_n, logs=False):\n",
    "        pd.DataFrame(inputs).to_csv(f'mapreducetemp/part-tmp-{thread_n}.csv', index=False, header=['key', 'value'])\n",
    "        if logs:\n",
    "            print(f'thread {thread_n} created tmp ')\n",
    "        return True\n",
    "    \n",
    "    def write_final(inputs, thread_n, logs=False):\n",
    "        pd.DataFrame({'key': [inputs[0]], 'value': [inputs[1]]}).to_csv(f'mapreducefinal/part-{thread_n}-final.csv', index=False)\n",
    "        if logs:\n",
    "            print(f'thread {thread_n} created final ')\n",
    "        return True\n",
    "    \n",
    "    def check_threads(threads):\n",
    "        for thread in threads:\n",
    "            if not thread.result():  # Should be True because write_tmp() and write_final() returns True\n",
    "                return 'MapReduce Failed'\n",
    "    \n",
    "    @staticmethod\n",
    "    def execute(input_data, map_function, reduce_function, conn, logs=False):\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            threads = []\n",
    "            for i, csv in enumerate(input_data, 1):\n",
    "                file = executor.submit(map_function, csv)\n",
    "                threads.append(executor.submit(MapReduceEngine.write_tmp, file.result(), i, logs))\n",
    "        MapReduceEngine.check_threads(threads)\n",
    "        if logs:\n",
    "            print('**all threads finished tmp**\\n')\n",
    "        \n",
    "        for file in os.listdir('mapreducetemp/'):\n",
    "            pd.read_csv(f'mapreducetemp/{file}').to_sql('temp_results', conn, if_exists='append', index=False)\n",
    "            \n",
    "        sql = '''SELECT key, GROUP_CONCAT(value)\n",
    "         FROM temp_results\n",
    "         GROUP BY key\n",
    "         ORDER BY key'''\n",
    "        reduce_input = list(pd.read_sql(sql, conn).to_records(index=False))\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            threads_2 = []\n",
    "            for i, reduce in enumerate(reduce_input, 1):\n",
    "                file = executor.submit(reduce_function, reduce[0], reduce[1])\n",
    "                threads_2.append(executor.submit(MapReduceEngine.write_final, file.result(), i, logs))\n",
    "        MapReduceEngine.check_threads(threads_2)\n",
    "        return '**MapReduced Completed**'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 3\n",
    "## Implement the MapReduce Inverted index of the JSON documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function `inverted_map(document_name)` which reads the CSV document from the local disc and return a list that contains entries of the form (key_value, document name).\n",
    "\n",
    "For example, if myCSV4.csv document has values like:  \n",
    "`{‘firstname’:’John’,‘secondname’:’Rambo’,‘city’:’Palo Alto’}`\n",
    "\n",
    "Then `inverted_map(‘myCSV4.csv’)` function will return a list:  \n",
    "`[(‘firstname_John’,’ myCSV4.csv’),(‘secondname_Rambo’,’ myCSV4.csv’), (‘city_Palo Alto’,’ myCSV4.csv’)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** - We first store the title row of the file, then iterate over the remaining rows, mapping each of a row's entries to the correspondeing element in the title using a dictionary. Only then we append the required tuple into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_map(document_name):\n",
    "    lst = []\n",
    "    with open(document_name, 'r') as f:\n",
    "        title_row = f.readline().strip().split(',')\n",
    "        for line in f:\n",
    "            dic = dict(zip(title_row, line.strip().split(',')))\n",
    "            lst.extend([(f'{key}_{value}', document_name) for key, value in dic.items()])\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a reduce function `inverted_reduce(value, documents)`, where the field “documents” contains a list of all CSV documents per given value.   \n",
    "This list might have duplicates.   \n",
    "Reduce function will return new list without duplicates.\n",
    "\n",
    "For example,  \n",
    "calling the function `inverted_reduce(‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’)`   \n",
    "will return a list `[‘firstname_Albert’,’myCSV2.csv, myCSV5.csv,myCSV2.csv’]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** - In the following implementation, we used the `set` properties to filter out repeating values. Keep in mind that using sets doesn't maintain the original order. In that case though, we don't mind. Also, we removed the spaces by mapping the `str.strip` method on documents input.\n",
    "\n",
    "The reason for the spacing removal is to make the output of `'myCSV2.csv, myCSV2.csv'` to be the same as `'myCSV2.csv,myCSV2.csv'` for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_reduce(value, documents):\n",
    "    lst = [value]\n",
    "    lst.append(','.join(set(map(str.strip, list(documents.split(','))))))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "# Question 4\n",
    "## Testing Your MapReduce\n",
    "\n",
    "**Create Python list** `input_data` : `[‘myCSV1.csv’,.. ,‘myCSV20.csv’]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [f'myCSV{i}.csv' for i in range(1,21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit MapReduce as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**MapReduced Completed**\n"
     ]
    }
   ],
   "source": [
    "mapreduce = MapReduceEngine()\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_reduce, conn)  # You can try with logs=True\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that `MapReduce Completed` should be printed and `mapreducefinal` folder should contain the result files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use python to delete all temporary data from mapreducetemp folder and delete SQLite database:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('mapreducetemp/', ignore_errors=True)\n",
    "os.mkdir('mapreducetemp')\n",
    "conn.close()\n",
    "os.remove('mydb.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus - not required\n",
    "We do the same thing, but with `logs=True` to see the threads in action. We can actually see that the order which the threads finish isn't neccessarly consistent (e.g thread 3 can finish before thread 2). That really makes it feel like different machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('mydb.db')\n",
    "c = conn.execute('''CREATE TABLE IF NOT EXISTS temp_results(\n",
    "                    key VARCHAR(20),\n",
    "                    value VARCHAR(20)\n",
    "                    )''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 1 created tmp \n",
      "thread 2 created tmp \n",
      "thread 3 created tmp \n",
      "thread 4 created tmp \n",
      "thread 5 created tmp \n",
      "thread 6 created tmp thread 7 created tmp \n",
      "\n",
      "thread 8 created tmp \n",
      "thread 9 created tmp \n",
      "thread 10 created tmp \n",
      "thread 11 created tmp \n",
      "thread 12 created tmp \n",
      "thread 13 created tmp \n",
      "thread 14 created tmp \n",
      "thread 15 created tmp \n",
      "thread 16 created tmp \n",
      "thread 17 created tmp \n",
      "thread 18 created tmp \n",
      "thread 19 created tmp \n",
      "thread 20 created tmp \n",
      "**all threads finished tmp**\n",
      "\n",
      "thread 1 created final thread 2 created final \n",
      "\n",
      "thread 3 created final \n",
      "thread 6 created final thread 4 created final \n",
      "thread 5 created final \n",
      "\n",
      "thread 7 created final \n",
      "thread 8 created final \n",
      "thread 9 created final \n",
      "thread 10 created final \n",
      "thread 12 created final thread 11 created final \n",
      "\n",
      "thread 13 created final \n",
      "thread 14 created final \n",
      "thread 16 created final thread 15 created final \n",
      "\n",
      "thread 17 created final \n",
      "thread 18 created final thread 19 created final \n",
      "\n",
      "thread 20 created final \n",
      "thread 22 created final \n",
      "thread 21 created final \n",
      "thread 23 created final \n",
      "**MapReduced Completed**\n"
     ]
    }
   ],
   "source": [
    "mapreduce = MapReduceEngine()\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_reduce, conn, logs=True)\n",
    "print(status)\n",
    "\n",
    "# Deleting again\n",
    "shutil.rmtree('mapreducetemp/', ignore_errors=True)\n",
    "os.mkdir('mapreducetemp')\n",
    "conn.close()\n",
    "os.remove('mydb.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Question 5\n",
    "# Final Thoughts\n",
    "\n",
    "The phase where `MapReduceEngine` reads all temporary files generated by maps and sort them to provide each reducer a specific key is called the **shuffle step**.\n",
    "\n",
    "Please explain **clearly** what would be the main problem of MapReduce when processing Big Data, if there is no shuffle step at all, meaning reducers will directly read responses from the mappers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "The main problem when processing Big Data via MapReduce without shuffle is you may have some repetition of (key, value) pairs after the reduce step. That's because the shuffle step should map every unique key to a single reducer.  \n",
    "\n",
    "For example if you wish to compute unique word counts in a document without shuffle, then several reducers may hold values for the same key. An additional aggregation would be required to yield total word count for the repeated keys which in turn would increase latency.\n",
    "\n",
    "Shuffling keys between reducers is crucial for guaranteeing the reduce step will have completely aggregated all keys in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "Good Luck :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
